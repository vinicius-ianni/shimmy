#![allow(clippy::too_many_arguments)]
use anyhow::Result;
use async_trait::async_trait;
use tracing::warn;

use super::{GenOptions, InferenceEngine, LoadedModel, ModelSpec};

/// Smart thread detection optimized for inference performance
/// Matches Ollama's approach: use physical cores with intelligent limits
#[allow(dead_code)]
fn get_optimal_thread_count() -> i32 {
    let total_cores = std::thread::available_parallelism()
        .map(|n| n.get() as i32)
        .unwrap_or(4);

    // Ollama logic: Use physical cores, not logical (hyperthreading) cores
    // Intel i7 typically has 4-8 physical cores but 8-16 logical cores
    let physical_cores = match total_cores {
        1..=2 => total_cores,               // Single/dual core: use all
        3..=4 => total_cores,               // Quad core: use all physical
        5..=8 => (total_cores / 2).max(4),  // 6-8 core: assume hyperthreading, use physical
        9..=16 => (total_cores / 2).max(6), // 8+ core: definitely hyperthreaded, use ~half
        _ => 8,                             // High-end systems: cap at 8 threads for stability
    };

    // Further optimization: leave some cores for system
    let optimal = match physical_cores {
        1..=2 => physical_cores,
        3..=4 => physical_cores - 1, // Leave 1 core for system
        5..=8 => physical_cores - 2, // Leave 2 cores for system
        _ => physical_cores * 3 / 4, // Use 75% of physical cores
    }
    .max(1); // Always use at least 1 thread

    tracing::info!(
        "Threading: {} total cores detected, using {} optimal threads",
        total_cores,
        optimal
    );
    optimal
}

#[cfg(feature = "llama")]
use std::sync::Mutex;
use tracing::info;

#[cfg(feature = "llama")]
use std::sync::OnceLock;

#[cfg(feature = "llama")]
static LLAMA_BACKEND: OnceLock<Result<shimmy_llama_cpp_2::llama_backend::LlamaBackend, String>> =
    OnceLock::new();

#[cfg(feature = "llama")]
fn get_or_init_backend() -> Result<&'static shimmy_llama_cpp_2::llama_backend::LlamaBackend> {
    use anyhow::anyhow;

    let result = LLAMA_BACKEND.get_or_init(|| {
        info!("Initializing llama.cpp backend (first model load)");
        shimmy_llama_cpp_2::llama_backend::LlamaBackend::init()
            .map_err(|e| format!("Failed to initialize llama backend: {}", e))
    });

    result.as_ref().map_err(|e| anyhow!("{}", e))
}

pub struct LlamaEngine {
    gpu_backend: GpuBackend,
    moe_config: MoeConfig,
    #[cfg(feature = "llama")]
    loaded: std::sync::Arc<
        std::sync::Mutex<std::collections::HashMap<String, std::sync::Arc<LlamaLoaded>>>,
    >,
}

impl Default for LlamaEngine {
    fn default() -> Self {
        Self {
            gpu_backend: GpuBackend::default(),
            moe_config: MoeConfig::default(),
            #[cfg(feature = "llama")]
            loaded: std::sync::Arc::new(std::sync::Mutex::new(std::collections::HashMap::new())),
        }
    }
}

#[derive(Debug, Clone, Default)]
struct MoeConfig {
    // These fields are only used when the "llama" feature is enabled,
    // in model loading code that configures MoE CPU offloading.
    // They appear "dead" when compiling without llama feature.
    #[allow(dead_code)]
    cpu_moe_all: bool,
    #[allow(dead_code)]
    n_cpu_moe: Option<usize>,
}

#[derive(Debug, Clone, Default)]
pub enum GpuBackend {
    #[default]
    Cpu,
    #[cfg(feature = "llama-cuda")]
    Cuda,
    #[cfg(feature = "llama-vulkan")]
    Vulkan,
    #[cfg(feature = "llama-opencl")]
    OpenCL,
}

impl GpuBackend {
    /// Parse GPU backend from CLI string
    fn from_string(s: &str) -> Self {
        match s.to_lowercase().as_str() {
            "auto" => Self::detect_best(),
            "cpu" => GpuBackend::Cpu,
            #[cfg(feature = "llama-cuda")]
            "cuda" => GpuBackend::Cuda,
            #[cfg(feature = "llama-vulkan")]
            "vulkan" => GpuBackend::Vulkan,
            #[cfg(feature = "llama-opencl")]
            "opencl" => GpuBackend::OpenCL,
            // Better error messages for backends not compiled in
            #[cfg(not(feature = "llama-cuda"))]
            "cuda" => {
                tracing::warn!("CUDA backend requested but not enabled at compile time. Rebuild with --features llama-cuda");
                Self::detect_best()
            }
            #[cfg(not(feature = "llama-vulkan"))]
            "vulkan" => {
                tracing::warn!("Vulkan backend requested but not enabled at compile time. Rebuild with --features llama-vulkan");
                Self::detect_best()
            }
            #[cfg(not(feature = "llama-opencl"))]
            "opencl" => {
                tracing::warn!("OpenCL backend requested but not enabled at compile time. Rebuild with --features llama-opencl");
                Self::detect_best()
            }
            _ => {
                tracing::warn!("Unknown GPU backend '{}', using auto-detect", s);
                Self::detect_best()
            }
        }
    }

    /// Detect the best available GPU backend for this system
    fn detect_best() -> Self {
        #[cfg(feature = "llama-cuda")]
        {
            if Self::is_cuda_available() {
                info!("CUDA GPU detected, using CUDA backend");
                return GpuBackend::Cuda;
            }
        }

        #[cfg(feature = "llama-vulkan")]
        {
            if Self::is_vulkan_available() {
                info!("Vulkan GPU detected, using Vulkan backend");
                return GpuBackend::Vulkan;
            }
        }

        #[cfg(feature = "llama-opencl")]
        {
            if Self::is_opencl_available() {
                info!("OpenCL GPU detected, using OpenCL backend");
                return GpuBackend::OpenCL;
            }
        }

        info!("No GPU acceleration available, using CPU backend");
        GpuBackend::Cpu
    }

    #[cfg(feature = "llama-cuda")]
    fn is_cuda_available() -> bool {
        std::process::Command::new("nvidia-smi")
            .output()
            .map(|output| output.status.success())
            .unwrap_or(false)
    }

    #[cfg(feature = "llama-vulkan")]
    fn is_vulkan_available() -> bool {
        // Check for Vulkan loader and runtime
        // Try vulkaninfo first (most reliable)
        if std::process::Command::new("vulkaninfo")
            .arg("--summary")
            .output()
            .map(|output| output.status.success())
            .unwrap_or(false)
        {
            return true;
        }

        // Fallback: assume available if feature is enabled
        // (Vulkan may be present even without vulkaninfo tool)
        true
    }

    #[cfg(feature = "llama-opencl")]
    fn is_opencl_available() -> bool {
        // Check for OpenCL runtime using clinfo
        if std::process::Command::new("clinfo")
            .output()
            .map(|output| output.status.success())
            .unwrap_or(false)
        {
            return true;
        }

        // On Windows, try alternative detection
        #[cfg(target_os = "windows")]
        {
            // Check for OpenCL.dll in system paths
            if std::process::Command::new("where")
                .arg("OpenCL.dll")
                .output()
                .map(|output| output.status.success())
                .unwrap_or(false)
            {
                return true;
            }
        }

        // Fallback: assume available if feature is enabled
        true
    }

    /// Get the number of layers to offload to GPU
    #[allow(dead_code)]
    pub fn gpu_layers(&self) -> u32 {
        match self {
            GpuBackend::Cpu => 0, // No GPU offloading for CPU backend
            #[cfg(feature = "llama-cuda")]
            GpuBackend::Cuda => 999, // Offload all layers to CUDA
            #[cfg(feature = "llama-vulkan")]
            GpuBackend::Vulkan => 999, // Offload all layers to Vulkan
            #[cfg(feature = "llama-opencl")]
            GpuBackend::OpenCL => 999, // Offload all layers to OpenCL
        }
    }
}

impl LlamaEngine {
    pub fn new() -> Self {
        Self {
            gpu_backend: GpuBackend::detect_best(),
            moe_config: MoeConfig::default(),
            #[cfg(feature = "llama")]
            loaded: std::sync::Arc::new(std::sync::Mutex::new(std::collections::HashMap::new())),
        }
    }

    /// Create engine with specific GPU backend from CLI
    #[allow(dead_code)]
    pub fn new_with_backend(backend_str: Option<&str>) -> Self {
        let gpu_backend = backend_str
            .map(GpuBackend::from_string)
            .unwrap_or_else(GpuBackend::detect_best);

        // Set environment variables for GPU backend before any backend initialization
        Self::configure_gpu_environment(&gpu_backend);

        info!("GPU backend configured: {:?}", gpu_backend);

        Self {
            gpu_backend,
            moe_config: MoeConfig::default(),
            #[cfg(feature = "llama")]
            loaded: std::sync::Arc::new(std::sync::Mutex::new(std::collections::HashMap::new())),
        }
    }

    /// Configure environment variables for GPU backend
    fn configure_gpu_environment(gpu_backend: &GpuBackend) {
        match gpu_backend {
            #[cfg(feature = "llama-cuda")]
            GpuBackend::Cuda => {
                std::env::set_var("GGML_CUDA", "1");
                info!("Set GGML_CUDA=1 for CUDA backend");
            }
            #[cfg(feature = "llama-vulkan")]
            GpuBackend::Vulkan => {
                std::env::set_var("GGML_VULKAN", "1");
                info!("Set GGML_VULKAN=1 for Vulkan backend");
                #[cfg(target_os = "windows")]
                {
                    // On Windows, Vulkan might need ICD setup
                    if std::env::var("VK_ICD_FILENAMES").is_err() {
                        info!("Vulkan ICD not configured - GPU acceleration may not work");
                    }
                }
            }
            #[cfg(feature = "llama-opencl")]
            GpuBackend::OpenCL => {
                std::env::set_var("GGML_OPENCL", "1");
                // Set defaults if not already set
                if std::env::var("GGML_OPENCL_PLATFORM").is_err() {
                    std::env::set_var("GGML_OPENCL_PLATFORM", "0");
                    info!("Set GGML_OPENCL_PLATFORM=0 (default)");
                }
                if std::env::var("GGML_OPENCL_DEVICE").is_err() {
                    std::env::set_var("GGML_OPENCL_DEVICE", "0");
                    info!("Set GGML_OPENCL_DEVICE=0 (default)");
                }
                info!("Configured OpenCL environment variables");
            }
            GpuBackend::Cpu => {
                // No special environment setup needed for CPU
            }
        }
    }

    /// Set MoE CPU offloading configuration
    #[allow(dead_code)]
    pub fn with_moe_config(mut self, cpu_moe_all: bool, n_cpu_moe: Option<usize>) -> Self {
        self.moe_config = MoeConfig {
            cpu_moe_all,
            n_cpu_moe,
        };
        self
    }

    /// Calculate adaptive batch size based on context length to prevent GGML assert failures
    /// with large prompts (Issue #140)
    fn calculate_adaptive_batch_size(ctx_len: usize) -> u32 {
        // Base batch size for smaller contexts
        const BASE_BATCH_SIZE: u32 = 2048;

        // For larger contexts, scale up the batch size
        // Use context length as minimum, but cap at reasonable limits
        let adaptive_size = ctx_len.max(BASE_BATCH_SIZE as usize);

        // Cap at 8192 to prevent excessive memory usage while handling large contexts
        // This allows for contexts up to 8192 tokens with large prompts
        let capped_size = adaptive_size.min(8192);

        tracing::info!(
            "Batch size: {} (context: {}, adaptive calculation for large prompt support)",
            capped_size,
            ctx_len
        );

        capped_size as u32
    }

    /// Get information about the current GPU backend configuration
    #[allow(dead_code)]
    pub fn get_backend_info(&self) -> String {
        match self.gpu_backend {
            GpuBackend::Cpu => "CPU".to_string(),
            #[cfg(feature = "llama-cuda")]
            GpuBackend::Cuda => "CUDA".to_string(),
            #[cfg(feature = "llama-vulkan")]
            GpuBackend::Vulkan => "Vulkan".to_string(),
            #[cfg(feature = "llama-opencl")]
            GpuBackend::OpenCL => "OpenCL".to_string(),
        }
    }
}

/// Helper function to find the projector blob for Ollama vision models
#[cfg(feature = "llama")]
fn find_ollama_projector_blob(model_name: &str) -> Option<std::path::PathBuf> {
    fn projector_debug_enabled() -> bool {
        match std::env::var("SHIMMY_OLLAMA_PROJECTOR_DEBUG") {
            Ok(v) => {
                let v = v.trim();
                !v.is_empty() && v != "0" && v.to_lowercase() != "false"
            }
            Err(_) => false,
        }
    }

    // For Ollama models, we need to find the projector blob
    // This is a bit hacky - we run `ollama show <model> --modelfile` and parse the output
    // to find the second FROM statement which should be the projector

    // Extract the actual model name (remove registry prefix if present)
    let actual_model_name = if model_name.contains("registry.ollama.ai/library/") {
        model_name
            .strip_prefix("registry.ollama.ai/library/")?
            .replace('/', ":")
    } else {
        model_name.to_string()
    };

    if projector_debug_enabled() {
        eprintln!(
            "DEBUG: Looking for projector for model: {}",
            actual_model_name
        );
    }

    // Run ollama show to get the modelfile
    let output = std::process::Command::new("ollama")
        .args(["show", actual_model_name.as_str(), "--modelfile"])
        .output()
        .ok()?;

    if !output.status.success() {
        if projector_debug_enabled() {
            eprintln!("DEBUG: ollama show failed");
        }
        return None;
    }

    let modelfile = String::from_utf8_lossy(&output.stdout);
    if projector_debug_enabled() {
        eprintln!(
            "DEBUG: Received modelfile output ({} bytes)",
            modelfile.len()
        );
    }

    // Parse the FROM statements
    let mut from_lines = modelfile
        .lines()
        .filter(|line| line.trim().starts_with("FROM "))
        .map(|line| line.trim().strip_prefix("FROM ").unwrap_or(line.trim()));

    // Skip the first FROM (model), take the second (projector)
    let _model_from = from_lines.next()?;
    let projector_from = from_lines.next()?;

    if projector_debug_enabled() {
        eprintln!("DEBUG: Found projector: {}", projector_from);
    }

    Some(std::path::PathBuf::from(projector_from))
}

#[async_trait]
impl InferenceEngine for LlamaEngine {
    async fn load(&self, spec: &ModelSpec) -> Result<Box<dyn LoadedModel>> {
        #[cfg(feature = "llama")]
        {
            use anyhow::anyhow;
            use shimmy_llama_cpp_2 as llama;
            use std::num::NonZeroU32;
            use std::sync::Arc;

            // Fast-path: return cached model if already loaded
            if let Ok(cache) = self.loaded.lock() {
                if let Some(model) = cache.get(&spec.name) {
                    return Ok(Box::new(CachedLlamaLoaded {
                        inner: Arc::clone(model),
                    }));
                }
            }

            // Use global singleton backend (fixes Issue #128: BackendAlreadyInitialized)
            let be = get_or_init_backend()?;

            // Configure GPU acceleration based on backend
            let n_gpu_layers = self.gpu_backend.gpu_layers();
            info!(
                "Loading model with {} GPU layers ({:?} backend)",
                n_gpu_layers, self.gpu_backend
            );

            let mut model_params =
                llama::model::params::LlamaModelParams::default().with_n_gpu_layers(n_gpu_layers);

            // Apply MoE CPU offloading if configured
            // Enable MoE CPU offloading (Issue #108 fix)
            if let Some(n) = self.moe_config.n_cpu_moe {
                info!("MoE: Offloading first {} expert layers to CPU", n);
                model_params = model_params.with_n_cpu_moe(n);
            } else if self.moe_config.cpu_moe_all {
                info!("MoE: Offloading ALL expert tensors to CPU (saves ~80-85% VRAM)");
                model_params = model_params.with_cpu_moe_all();
            }

            // Attempt to load the model with better error handling
            let model = match llama::model::LlamaModel::load_from_file(
                be,
                &spec.base_path,
                &model_params,
            ) {
                Ok(model) => model,
                Err(e) => {
                    // Check if this looks like a memory allocation failure
                    let error_msg = format!("{}", e);
                    if error_msg.contains("failed to allocate")
                        || error_msg.contains("CPU_REPACK buffer")
                    {
                        let file_size = std::fs::metadata(&spec.base_path)
                            .map(|m| m.len())
                            .unwrap_or(0);
                        let size_gb = file_size as f64 / 1_024_000_000.0;

                        return Err(anyhow!(
                            "Memory allocation failed for model {} ({:.1}GB). \n\
                            ðŸ’¡ Possible solutions:\n\
                            â€¢ Use a smaller model (7B instead of 14B parameters)\n\
                            â€¢ Add more system RAM (model needs ~{}GB)\n\
                            â€¢ Enable model quantization (Q4_K_M, Q5_K_M)\n\
                            â€¢ MoE CPU offloading is temporarily disabled (Issue #108)\n\
                            Original error: {}",
                            spec.base_path.display(),
                            size_gb,
                            (size_gb * 1.5) as u32, // Rough estimate of RAM needed
                            e
                        ));
                    }

                    // Re-throw other errors as-is
                    return Err(e.into());
                }
            };
            let ctx_params = llama::context::params::LlamaContextParams::default()
                .with_n_ctx(NonZeroU32::new(spec.ctx_len as u32))
                .with_n_batch(Self::calculate_adaptive_batch_size(spec.ctx_len))
                .with_n_ubatch(512)
                .with_n_threads(spec.n_threads.unwrap_or_else(get_optimal_thread_count))
                .with_n_threads_batch(spec.n_threads.unwrap_or_else(get_optimal_thread_count));
            let ctx_tmp = model.new_context(be, ctx_params)?;
            if let Some(ref lora) = spec.lora_path {
                // Check if it's a SafeTensors file and convert if needed
                let lora_path = if lora.extension().and_then(|s| s.to_str()) == Some("safetensors")
                {
                    // For now, provide helpful error message for SafeTensors files
                    return Err(anyhow!(
                        "SafeTensors LoRA detected: {}. Please convert to GGUF format first.",
                        lora.display()
                    ));
                } else {
                    lora.clone()
                };

                let mut adapter = model.lora_adapter_init(&lora_path)?;
                ctx_tmp
                    .lora_adapter_set(&mut adapter, 1.0)
                    .map_err(|e| anyhow!("lora set: {e:?}"))?;
                info!(adapter=%lora_path.display(), "LoRA adapter attached");
            }

            // Vision models: record projector path, defer loading to external mtmd CLI
            let projector_path = if spec.name.to_lowercase().contains("minicpm")
                || spec.name.to_lowercase().contains("vision")
            {
                // Prefer local mmproj if present
                let mut projector_path = spec.base_path.with_file_name("mmproj-model-f16.gguf");

                // For Ollama models, locate the projector blob via modelfile
                if !projector_path.exists() && spec.base_path.to_string_lossy().contains("blobs") {
                    if let Some(projector_blob) = find_ollama_projector_blob(&spec.name) {
                        projector_path = projector_blob;
                        info!("Found Ollama projector: {}", projector_path.display());
                    } else {
                        warn!("Failed to find Ollama projector for model: {}", spec.name);
                    }
                }

                if projector_path.exists() {
                    info!(
                        "Vision projector path recorded: {}",
                        projector_path.display()
                    );
                    Some(projector_path)
                } else {
                    warn!(
                        "Vision model detected but no projector found at {}",
                        projector_path.display()
                    );
                    None
                }
            } else {
                None
            };

            // Store both model and context together to maintain proper lifetimes
            // The context lifetime is tied to &model; storing both in the same struct ensures safety
            let ctx: llama::context::LlamaContext<'static> =
                unsafe { std::mem::transmute(ctx_tmp) };
            let loaded = Arc::new(LlamaLoaded {
                model,
                ctx: Mutex::new(ctx),
                projector: None,
                model_path: spec.base_path.clone(),
                projector_path,
            });

            // Cache for reuse (best-effort; ignore poisoning)
            if let Ok(mut cache) = self.loaded.lock() {
                cache.insert(spec.name.clone(), Arc::clone(&loaded));
            }

            Ok(Box::new(CachedLlamaLoaded { inner: loaded }))
        }
        #[cfg(not(feature = "llama"))]
        {
            let _ = spec; // silence unused warning
            Ok(Box::new(LlamaFallback))
        }
    }
}

#[cfg(feature = "llama")]
struct LlamaLoaded {
    model: shimmy_llama_cpp_2::model::LlamaModel,
    ctx: Mutex<shimmy_llama_cpp_2::context::LlamaContext<'static>>,
    #[allow(dead_code)]
    projector: Option<shimmy_llama_cpp_2::model::LlamaModel>, // For vision models
    model_path: std::path::PathBuf,
    projector_path: Option<std::path::PathBuf>,
}

#[cfg(feature = "llama")]
struct CachedLlamaLoaded {
    inner: std::sync::Arc<LlamaLoaded>,
}

#[cfg(feature = "llama")]
// The llama.cpp context & model use raw pointers internally and are !Send by default.
// We wrap access in a Mutex and only perform FFI calls while holding the lock, so it's
// sound to mark the container Send + Sync for our usage (single-threaded mutable access).
unsafe impl Send for LlamaLoaded {}
#[cfg(feature = "llama")]
unsafe impl Sync for LlamaLoaded {}

#[cfg(feature = "llama")]
#[async_trait]
impl LoadedModel for LlamaLoaded {
    async fn generate(
        &self,
        prompt: &str,
        opts: GenOptions,
        mut on_token: Option<Box<dyn FnMut(String) + Send>>,
    ) -> Result<String> {
        use shimmy_llama_cpp_2::{
            llama_batch::LlamaBatch,
            model::{AddBos, Special},
            sampling::LlamaSampler,
        };
        let mut ctx = self
            .ctx
            .lock()
            .map_err(|e| anyhow::anyhow!("Failed to lock context: {}", e))?;
        let tokens = self.model.str_to_token(prompt, AddBos::Always)?;

        // Create batch with explicit logits configuration
        let mut batch = LlamaBatch::new(tokens.len(), 1);
        for (i, &token) in tokens.iter().enumerate() {
            // Only request logits for the last token in the initial batch
            let logits = i == tokens.len() - 1;
            batch.add(token, i as i32, &[0], logits)?;
        }
        ctx.decode(&mut batch)?;

        let mut sampler = LlamaSampler::chain_simple([
            LlamaSampler::temp(opts.temperature),
            LlamaSampler::top_p(opts.top_p, 1),
            LlamaSampler::top_k(opts.top_k),
            // API changed order: (repeat_last_n, freq_penalty, presence_penalty, penalty)
            LlamaSampler::penalties(64, 0.0, 0.0, opts.repeat_penalty),
            LlamaSampler::greedy(),
        ])
        .with_tokens(tokens.iter().copied());

        let mut out = String::new();
        let mut all_tokens = tokens;

        for _ in 0..opts.max_tokens {
            // Sample from the last (and only) position with logits
            let token = sampler.sample(&ctx, -1);
            if self.model.is_eog_token(token) {
                break;
            }
            // Use Plaintext to avoid re-tokenizing control tokens into special forms
            let piece = self.model.token_to_str(token, Special::Plaintext)?;
            out.push_str(&piece);

            // Check for stop tokens before emitting
            let should_stop = opts
                .stop_tokens
                .iter()
                .any(|stop_token| out.contains(stop_token));
            if should_stop {
                // Remove the stop token from the output, ensuring UTF-8 validity
                for stop_token in &opts.stop_tokens {
                    if let Some(pos) = out.rfind(stop_token) {
                        // Find the character boundary before the stop token
                        // This prevents truncating in the middle of multi-byte Unicode characters
                        let truncate_pos = out
                            .char_indices()
                            .take_while(|(byte_pos, _)| *byte_pos <= pos)
                            .last()
                            .map(|(byte_pos, _)| byte_pos)
                            .unwrap_or(0);
                        out.truncate(truncate_pos);
                        break;
                    }
                }
                break;
            }

            // Handle UTF-8 aware token streaming (Issue #139 fix)
            if let Some(cb) = on_token.as_mut() {
                cb(piece.clone());
            }

            let mut step = LlamaBatch::new(1, 1);
            step.add(token, all_tokens.len() as i32, &[0], true)?;
            ctx.decode(&mut step)?;
            all_tokens.push(token);
        }

        Ok(out)
    }

    async fn generate_vision(
        &self,
        image_data: &[u8],
        prompt: &str,
        _opts: GenOptions,
        _on_token: Option<Box<dyn FnMut(String) + Send>>,
    ) -> Result<String> {
        // Check if we have the model and projector paths for external CLI execution
        if self.model_path.as_os_str().is_empty() || self.projector_path.is_none() {
            return Err(anyhow::anyhow!(
                "This model does not support vision (missing model or projector paths)"
            ));
        }

        fn mtmd_np_from_env() -> u32 {
            let raw = std::env::var("SHIMMY_VISION_MTMD_NP")
                .or_else(|_| std::env::var("SHIMMY_MTMD_NP"))
                .ok();
            let parsed = raw
                .as_deref()
                .and_then(|s| s.trim().parse::<u32>().ok())
                .unwrap_or(1);
            parsed.clamp(1, 64)
        }

        fn mtmd_ctx_from_env() -> u32 {
            let raw = std::env::var("SHIMMY_VISION_MTMD_CTX")
                .or_else(|_| std::env::var("SHIMMY_MTMD_CTX"))
                .ok();
            let parsed = raw
                .as_deref()
                .and_then(|s| s.trim().parse::<u32>().ok())
                .unwrap_or(4096);
            parsed.clamp(512, 131072)
        }

        fn run_mtmd(
            mtmd_path: &std::path::Path,
            model_path: &std::path::Path,
            projector_path: &std::path::Path,
            image_path: &std::path::Path,
            prompt: &str,
            n_parallel: u32,
            ctx_size: u32,
        ) -> std::io::Result<std::process::Output> {
            std::process::Command::new(mtmd_path)
                // Avoid global LLAMA_ARG_* env vars overriding our explicit CLI flags.
                // In particular, LLAMA_ARG_N_PARALLEL can force n_seq_max high, shrinking
                // per-sequence context (n_ctx_seq) and triggering "failed to find a memory slot".
                .env_remove("LLAMA_ARG_N_PARALLEL")
                .env_remove("LLAMA_ARG_CTX_SIZE")
                .env_remove("LLAMA_ARG_BATCH")
                .env_remove("LLAMA_ARG_UBATCH")
                .env_remove("LLAMA_ARG_N_PREDICT")
                .env_remove("LLAMA_ARG_THREADS")
                .env_remove("LLAMA_ARG_THREADS_BATCH")
                .arg("-m")
                .arg(model_path)
                .arg("--mmproj")
                .arg(projector_path)
                .arg("-c")
                .arg(ctx_size.to_string())
                // MiniCPM-V often slices images; mtmd needs enough parallel sequence slots.
                .arg("-np")
                .arg(n_parallel.to_string())
                .arg("--image")
                .arg(image_path)
                .arg("-p")
                .arg(prompt)
                .arg("--temp")
                .arg("0.1")
                .output()
        }

        // Save image to temp file
        let temp_dir = std::env::temp_dir();
        // Preprocessed bytes are PNG, so write with .png to avoid decoder confusion.
        // Use a unique name per request to avoid collisions under concurrent /api/vision.
        static IMAGE_COUNTER: std::sync::atomic::AtomicU64 = std::sync::atomic::AtomicU64::new(0);
        let counter = IMAGE_COUNTER.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
        let nanos = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap_or_default()
            .as_nanos();
        let image_path = temp_dir.join(format!(
            "shimmy_vision_{}_{}_{}.png",
            std::process::id(),
            counter,
            nanos
        ));
        std::fs::write(&image_path, image_data)?;

        // Call mtmd CLI (consolidated multimodal runner)
        let workspace_dir = std::env::current_dir()?;
        let mtmd_candidates = [
            workspace_dir
                .join("llama-cpp-minicpm")
                .join("build-cuda")
                .join("bin")
                .join("Release")
                .join("llama-mtmd-cli.exe"),
            workspace_dir
                .join("llama-cpp-minicpm")
                .join("build")
                .join("bin")
                .join("Release")
                .join("llama-mtmd-cli.exe"),
            workspace_dir
                .join("llama-cpp-minicpm")
                .join("build")
                .join("bin")
                .join("Debug")
                .join("llama-mtmd-cli.exe"),
        ];

        let mtmd_path = mtmd_candidates
            .iter()
            .find(|p| p.exists())
            .cloned()
            .ok_or_else(|| anyhow::anyhow!("llama-mtmd-cli.exe not found; build with cmake --build build --target llama-mtmd-cli"))?;

        let projector_path = self.projector_path.as_ref().unwrap();
        let n_parallel = mtmd_np_from_env();
        let ctx_size = mtmd_ctx_from_env();
        let mut current_n_parallel = n_parallel;
        let mut current_ctx_size = ctx_size;

        let trace = std::env::var("SHIMMY_VISION_TRACE").is_ok();
        if trace {
            tracing::info!(
                target: "vision",
                stage = "mtmd_start",
                mtmd = %mtmd_path.display(),
                model = %self.model_path.display(),
                mmproj = %projector_path.display(),
                image = %image_path.display(),
                n_parallel = current_n_parallel,
                ctx_size = current_ctx_size,
                prompt_chars = prompt.len(),
                image_bytes = image_data.len(),
                "invoking mtmd"
            );
        }
        let mut output = run_mtmd(
            &mtmd_path,
            &self.model_path,
            projector_path,
            &image_path,
            prompt,
            current_n_parallel,
            current_ctx_size,
        )?;

        // Retry with more slots if mtmd reports the known MiniCPM "memory slot" failure.
        // This shows up as HTTP 500 in Shimmy if we don't handle it.
        if !output.status.success() {
            let stderr = String::from_utf8_lossy(&output.stderr);

            if trace {
                tracing::warn!(
                    target: "vision",
                    stage = "mtmd_fail",
                    status = %output.status,
                    stderr_chars = stderr.len(),
                    "mtmd failed"
                );
            }

            let is_slot_failure = stderr.contains("failed to find a memory slot")
                || stderr.contains("failed to decode token")
                || stderr.contains("failed to decode text")
                || stderr.contains("failed to decode image");

            // mtmd can fail when the prompt token count exceeds per-sequence context.
            // Example: "find_slot: n_tokens = 323 > size = 256".
            let find_slot_re = regex::Regex::new(r"find_slot: n_tokens = (\d+) > size = (\d+)")
                .expect("valid regex");

            // Attempt 1: prefer minimizing parallelism (maximizes per-seq context) and increasing ctx.
            if is_slot_failure {
                let retry_n_parallel = 1;
                let retry_ctx = current_ctx_size.max(8192);
                current_n_parallel = retry_n_parallel;
                current_ctx_size = retry_ctx;
                if trace {
                    tracing::info!(
                        target: "vision",
                        stage = "mtmd_retry",
                        attempt = 1,
                        n_parallel = current_n_parallel,
                        ctx_size = current_ctx_size,
                        "retrying mtmd"
                    );
                }
                output = run_mtmd(
                    &mtmd_path,
                    &self.model_path,
                    projector_path,
                    &image_path,
                    prompt,
                    current_n_parallel,
                    current_ctx_size,
                )?;

                // If it still fails with the same slot issue, try one more time with more slots.
                if !output.status.success() {
                    let stderr_retry = String::from_utf8_lossy(&output.stderr);
                    let still_slot_failure = stderr_retry.contains("failed to find a memory slot")
                        || stderr_retry.contains("failed to decode token")
                        || stderr_retry.contains("failed to decode text")
                        || stderr_retry.contains("failed to decode image");

                    if still_slot_failure {
                        let retry2_ctx = current_ctx_size.max(16384);
                        current_ctx_size = retry2_ctx;
                        if trace {
                            tracing::info!(
                                target: "vision",
                                stage = "mtmd_retry",
                                attempt = 2,
                                n_parallel = current_n_parallel,
                                ctx_size = current_ctx_size,
                                "retrying mtmd"
                            );
                        }
                        output = run_mtmd(
                            &mtmd_path,
                            &self.model_path,
                            projector_path,
                            &image_path,
                            prompt,
                            current_n_parallel,
                            current_ctx_size,
                        )?;
                    }
                }
            }

            // Attempt 2: if mtmd tells us n_tokens > size, increase -c to satisfy it.
            if !output.status.success() {
                let stderr2 = String::from_utf8_lossy(&output.stderr);
                if let Some(caps) = find_slot_re.captures(&stderr2) {
                    let n_tokens = caps
                        .get(1)
                        .and_then(|m| m.as_str().parse::<u32>().ok())
                        .unwrap_or(0);
                    // Want n_ctx_seq >= n_tokens + margin.
                    let needed_per_seq = (n_tokens + 128).max(512);
                    let needed_ctx = current_n_parallel
                        .saturating_mul(needed_per_seq)
                        .max(current_ctx_size);
                    let needed_ctx = needed_ctx.clamp(512, 131072);
                    current_ctx_size = needed_ctx;
                    if trace {
                        tracing::info!(
                            target: "vision",
                            stage = "mtmd_retry",
                            attempt = 3,
                            n_parallel = current_n_parallel,
                            ctx_size = current_ctx_size,
                            "retrying mtmd based on find_slot"
                        );
                    }
                    output = run_mtmd(
                        &mtmd_path,
                        &self.model_path,
                        projector_path,
                        &image_path,
                        prompt,
                        current_n_parallel,
                        current_ctx_size,
                    )?;
                }
            }
        }

        // Clean up temp file
        let _ = std::fs::remove_file(&image_path);

        if output.status.success() {
            if trace {
                tracing::info!(
                    target: "vision",
                    stage = "mtmd_ok",
                    stdout_chars = output.stdout.len(),
                    "mtmd succeeded"
                );
            }
            Ok(String::from_utf8_lossy(&output.stdout).to_string())
        } else {
            Err(anyhow::anyhow!(
                "minicpmv-cli failed: {}",
                String::from_utf8_lossy(&output.stderr)
            ))
        }
    }
}

#[cfg(feature = "llama")]
#[async_trait]
impl LoadedModel for CachedLlamaLoaded {
    async fn generate(
        &self,
        prompt: &str,
        opts: GenOptions,
        on_token: Option<Box<dyn FnMut(String) + Send>>,
    ) -> Result<String> {
        self.inner.generate(prompt, opts, on_token).await
    }

    async fn generate_vision(
        &self,
        image_data: &[u8],
        prompt: &str,
        opts: GenOptions,
        on_token: Option<Box<dyn FnMut(String) + Send>>,
    ) -> Result<String> {
        self.inner
            .generate_vision(image_data, prompt, opts, on_token)
            .await
    }
}

/// Fallback implementation when llama.cpp feature is not enabled
/// Returns informative message directing users to enable the feature
#[cfg(not(feature = "llama"))]
struct LlamaFallback;

#[cfg(not(feature = "llama"))]
#[async_trait]
impl LoadedModel for LlamaFallback {
    async fn generate(
        &self,
        prompt: &str,
        _opts: GenOptions,
        mut on_token: Option<Box<dyn FnMut(String) + Send>>,
    ) -> Result<String> {
        let fallback_msg =
            "Llama.cpp support not enabled. Build with --features llama for full functionality.";
        if let Some(cb) = on_token.as_mut() {
            cb(fallback_msg.to_string());
        }
        Ok(format!("[INFO] {} Input: {}", fallback_msg, prompt))
    }

    async fn generate_vision(
        &self,
        _image_data: &[u8],
        prompt: &str,
        _opts: GenOptions,
        mut on_token: Option<Box<dyn FnMut(String) + Send>>,
    ) -> Result<String> {
        let fallback_msg =
            "Llama.cpp support not enabled. Build with --features llama for vision functionality.";
        if let Some(cb) = on_token.as_mut() {
            cb(fallback_msg.to_string());
        }
        Ok(format!("[INFO] {} Input: {}", fallback_msg, prompt))
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_llama_engine_initialization() {
        let engine = LlamaEngine::new();
        // LlamaEngine has a GpuBackend field - just verify it can be created
        // Size depends on which GPU features are compiled in
        let _ = engine; // Suppress unused variable warning
    }

    #[test]
    fn test_gpu_backend_layer_offloading_logic() {
        // Issue #130: Verify CPU backend offloads 0 layers, GPU backends offload all layers

        let cpu_backend = GpuBackend::Cpu;
        assert_eq!(
            cpu_backend.gpu_layers(),
            0,
            "CPU backend should offload 0 layers to GPU"
        );

        #[cfg(feature = "llama-cuda")]
        {
            let cuda_backend = GpuBackend::Cuda;
            assert_eq!(
                cuda_backend.gpu_layers(),
                999,
                "CUDA backend should offload 999 layers to GPU"
            );
        }

        #[cfg(feature = "llama-vulkan")]
        {
            let vulkan_backend = GpuBackend::Vulkan;
            assert_eq!(
                vulkan_backend.gpu_layers(),
                999,
                "Vulkan backend should offload 999 layers to GPU"
            );
        }

        #[cfg(feature = "llama-opencl")]
        {
            let opencl_backend = GpuBackend::OpenCL;
            assert_eq!(
                opencl_backend.gpu_layers(),
                999,
                "OpenCL backend should offload 999 layers to GPU"
            );
        }
    }

    #[tokio::test]
    async fn test_model_loading_validation() {
        let _engine = LlamaEngine::new();
        let _spec = ModelSpec {
            name: "test".to_string(),
            base_path: "/nonexistent".into(),
            lora_path: None,
            template: Some("chatml".to_string()),
            ctx_len: 2048,
            n_threads: None,
        };

        // let result = engine.load(&spec).await; // Commented to avoid test file dependencies
        // assert!(result.is_err()); // Test spec structure instead
    }

    #[test]
    fn test_model_spec_validation() {
        let spec = ModelSpec {
            name: "valid".to_string(),
            base_path: "test.gguf".into(),
            lora_path: None,
            template: Some("chatml".to_string()),
            ctx_len: 4096,
            n_threads: Some(4),
        };

        assert_eq!(spec.name, "valid");
        assert_eq!(spec.ctx_len, 4096);
        assert!(spec.template.is_some());
    }
}
