# MoE CPU Offloading Stress Testing Protocol

## Overview

This document outlines comprehensive stress testing protocols for validating MoE models with CPU offloading across three validated architectures:

1. **GPT-OSS 20B**: 32 experts, 4 active per token
2. **Phi-3.5-MoE 41.9B**: 16 experts, 2 active per token
3. **DeepSeek MoE 16B**: 64 experts + 2 shared experts, 6 active per token

## Test Categories

### 1. Basic Functionality Tests âœ… COMPLETED
- [x] Model loading with CPU offloading
- [x] Basic generation (50-150 tokens)
- [x] Memory footprint validation
- [x] Expert tensor CPU assignment verification

### 2. Scale & Endurance Tests

#### 2.1 Long-Form Generation
- **Objective**: Test sustained generation over extended sequences
- **Tests**:
  - Generate 2000+ token responses
  - Multi-paragraph articles (5000+ tokens)
  - Continuous generation sessions (30+ minutes)
- **Metrics**: Tokens/second, memory stability, quality consistency

#### 2.2 Concurrent Load Testing
- **Objective**: Multiple simultaneous inference sessions
- **Tests**:
  - 3-5 parallel generation requests
  - Different prompt types per session
  - Mixed short/long generations
- **Metrics**: Throughput degradation, memory pressure, stability

#### 2.3 Context Window Stress
- **Objective**: Test full context window utilization
- **Tests**:
  - GPT-OSS: 131K context utilization
  - Phi-3.5-MoE: 128K context utilization
  - DeepSeek: 4K context utilization
- **Metrics**: Memory scaling, performance at max context

### 3. Expert Activation Pattern Analysis

#### 3.1 Expert Routing Verification
- **Objective**: Validate different prompts activate different experts
- **Tests**:
  - Code generation vs creative writing
  - Math problems vs language translation
  - Technical documentation vs casual conversation
- **Metrics**: Expert activation patterns, routing diversity

#### 3.2 Specialization Testing
- **Objective**: Verify expert specialization benefits
- **Tests**:
  - Domain-specific prompts (science, literature, code)
  - Cross-domain prompt mixing
  - Specialized vs general knowledge queries
- **Metrics**: Response quality, expert utilization efficiency

### 4. Production Simulation Tests

#### 4.1 Real-World Conversation Flows
- **Objective**: Simulate actual AI assistant usage
- **Tests**:
  - Multi-turn conversations (10+ exchanges)
  - Context-dependent follow-up questions
  - Topic switching within conversations
- **Metrics**: Context retention, response consistency, performance stability

#### 4.2 API Server Stress Testing
- **Objective**: Test shimmy server under load
- **Tests**:
  - HTTP API concurrent requests
  - WebSocket streaming sessions
  - SSE streaming performance
  - Mixed API endpoint usage
- **Metrics**: Response times, connection stability, throughput

### 5. Memory & Performance Benchmarks

#### 5.1 Memory Efficiency Validation
- **Objective**: Confirm CPU offloading benefits persist under stress
- **Tests**:
  - GPU memory monitoring during peak usage
  - CPU memory scaling patterns
  - Memory pressure recovery
- **Metrics**: Peak GPU usage, CPU memory growth, garbage collection

#### 5.2 Performance Profiling
- **Objective**: Identify bottlenecks and optimization opportunities
- **Tests**:
  - Token generation speed across context lengths
  - First token latency (TTFT)
  - Expert switching overhead
- **Metrics**: Tokens/second, latency distribution, CPU/GPU utilization

### 6. Quality & Correctness Tests

#### 6.1 Output Quality Consistency
- **Objective**: Ensure CPU offloading doesn't degrade quality
- **Tests**:
  - Identical prompts across test runs
  - Quality comparison vs GPU-only inference
  - Coherence across long generations
- **Metrics**: Response similarity, coherence scores, factual accuracy

#### 6.2 Mathematical & Logical Reasoning
- **Objective**: Test complex reasoning capabilities
- **Tests**:
  - Multi-step math problems
  - Logical puzzles and reasoning chains
  - Code generation and debugging
- **Metrics**: Accuracy rates, reasoning quality, code correctness

## Test Implementation Framework

### Automated Test Suite Components

1. **Benchmark Runner Script**
   - Configurable test parameters
   - Automated metrics collection
   - Result aggregation and reporting

2. **Memory Monitor**
   - GPU memory tracking (nvidia-smi integration)
   - CPU memory monitoring
   - Real-time usage graphs

3. **Performance Profiler**
   - Token generation timing
   - Expert activation logging
   - Bottleneck identification

4. **Quality Validator**
   - Response consistency checking
   - Output quality metrics
   - Regression detection

### Test Environment Requirements

- **Hardware**: NVIDIA GH200 with 97GB VRAM
- **Software**: shimmy feat/moe-cpu-offload branch
- **Models**: All three GGUF models with CPU offloading enabled
- **Monitoring**: htop, nvidia-smi, custom metrics collection

## Success Criteria

### Performance Thresholds
- **Token Generation**: >10 tokens/second sustained
- **Memory Efficiency**: <5GB GPU memory per model
- **Stability**: 8+ hour continuous operation
- **Quality**: >95% consistency with GPU-only baseline

### Scalability Requirements
- **Concurrent Sessions**: 3+ simultaneous without degradation
- **Context Scaling**: Linear memory growth only
- **Expert Utilization**: >70% of available experts used across diverse prompts

## Stress Test Scenarios

### Scenario 1: "AI Assistant Marathon"
- 8-hour continuous conversation simulation
- Multiple conversation threads
- Mixed prompt types (creative, technical, analytical)
- Memory monitoring throughout

### Scenario 2: "Expert Specialization Challenge"
- Prompts designed to activate different expert subsets
- Cross-domain knowledge integration
- Expert routing pattern analysis
- Quality assessment across domains

### Scenario 3: "Production Load Simulation"
- Realistic API usage patterns
- Burst traffic simulation
- Mixed request sizes and types
- Server stability under pressure

### Scenario 4: "Context Window Saturation"
- Gradually increase context until limits
- Monitor memory scaling behavior
- Performance degradation patterns
- Recovery after context reset

## Reporting Framework

### Real-Time Dashboards
- Live performance metrics
- Memory usage graphs
- Expert activation heatmaps
- Quality trend analysis

### Comprehensive Reports
- Executive summary with key findings
- Detailed performance breakdowns
- Comparative analysis across models
- Recommendations for optimization

### Regression Testing
- Baseline establishment for each model
- Automated regression detection
- Performance trend monitoring
- Quality consistency tracking

## Future Enhancements

### Advanced Testing Scenarios
- Multi-model expert sharing experiments
- Dynamic expert offloading optimization
- Hybrid CPU/GPU expert placement
- Real-time expert routing adaptation

### Integration Testing
- shimmy integration with other tools
- API compatibility validation
- Plugin architecture stress testing
- Deployment scenario validation

---

This protocol provides comprehensive validation that MoE CPU offloading is production-ready for real-world AI assistant workloads, demonstrating both technical innovation and practical utility.
